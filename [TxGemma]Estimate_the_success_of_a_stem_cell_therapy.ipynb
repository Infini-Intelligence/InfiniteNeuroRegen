{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO+ax0PTzpVd0pctZOmi0We",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianawin/InfiniteNeuroRegen/blob/main/%5BTxGemma%5DEstimate_the_success_of_a_stem_cell_therapy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugGlGbnlvWJT"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "import os, re, json\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "genai.configure(api_key=userdata.get(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# Install dependencies\n",
        "!pip install --upgrade --quiet accelerate bitsandbytes huggingface_hub transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load prompt template\n",
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "tdc_prompts_filepath = hf_hub_download(\n",
        "    repo_id=\"google/txgemma-2b-predict\",\n",
        "    filename=\"tdc_prompts.json\",\n",
        ")\n",
        "\n",
        "with open(tdc_prompts_filepath, \"r\") as f:\n",
        "    tdc_prompts_json = json.load(f)"
      ],
      "metadata": {
        "id": "SfysFpGlvYI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "PREDICT_VARIANT = \"2b-predict\"  # @param [\"2b-predict\", \"9b-predict\", \"27b-predict\"]\n",
        "CHAT_VARIANT = \"9b-chat\" # @param [\"9b-chat\", \"27b-chat\"]\n",
        "USE_CHAT = True # @param {type: \"boolean\"}\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "predict_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{PREDICT_VARIANT}\")\n",
        "predict_model = AutoModelForCausalLM.from_pretrained(\n",
        "    f\"google/txgemma-{PREDICT_VARIANT}\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "if USE_CHAT:\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(f\"google/txgemma-{CHAT_VARIANT}\")\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        f\"google/txgemma-{CHAT_VARIANT}\",\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "    )"
      ],
      "metadata": {
        "id": "-WRB3qSiveck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example task and input\n",
        "task_name = \"BBB_Martins\"\n",
        "input_type = \"{Drug SMILES}\"\n",
        "drug_smiles = \"CN1C(=O)CN=C(C2=CCCCC2)c2cc(Cl)ccc21\"\n",
        "TDC_PROMPT = tdc_prompts_json[task_name].replace(input_type, drug_smiles)\n",
        "\n",
        "def txgemma_predict(prompt):\n",
        "    input_ids = predict_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = predict_model.generate(**input_ids, max_new_tokens=8)\n",
        "    return predict_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def txgemma_chat(prompt):\n",
        "    input_ids = chat_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = chat_model.generate(**input_ids, max_new_tokens=32)\n",
        "    return chat_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Prediction model response: {txgemma_predict(TDC_PROMPT)}\")\n",
        "if USE_CHAT: print(f\"Chat model response: {txgemma_chat(TDC_PROMPT)}\")"
      ],
      "metadata": {
        "id": "radPTRYqFHBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will allow us to extract content from inside of ticks\n",
        "def extract_prompt(text, word):\n",
        "    code_block_pattern = rf\"```{word}(.*?)```\"\n",
        "    code_blocks = re.findall(code_block_pattern, text, re.DOTALL)\n",
        "    extracted_code = \"\\n\".join(code_blocks).strip()\n",
        "    return extracted_code\n",
        "\n",
        "# This class will allow us to inferface with TxGemma\n",
        "class TxGemmaChatTool:\n",
        "    def __init__(self):\n",
        "      self.tool_name = \"Chat Tool\"\n",
        "\n",
        "    def use_tool(self, question):\n",
        "        # Here, we are submitting a question to TxGemma\n",
        "        response = txgemma_chat(question)\n",
        "        return response\n",
        "\n",
        "    def tool_is_used(self, query):\n",
        "        # This just checks to see if the tool call was evoked\n",
        "        return \"```TxGemmaChat\" in query\n",
        "\n",
        "    def process_query(self, query):\n",
        "        # Here, we clean to query to remove the tool call\n",
        "        return extract_prompt(query, word=\"TxGemmaChat\")\n",
        "\n",
        "    def instructions(self):\n",
        "        return (\n",
        "            \"=== TX-009 Task: Therapeutic Chat Tool Instructions ===\\n\"\n",
        "            \"### What This Tool Does\\n\"\n",
        "            \"The TxGemma Therapeutic Chat Tool allows the agent to ask domain-specific questions to a large language model \"\n",
        "            \"fine-tuned on therapeutic and biomedical datasets. It is particularly configured for TX-009 to:\\n\"\n",
        "            \"- Interpret trial metadata\\n\"\n",
        "            \"- Extract and rank top 5 contributing factors\\n\"\n",
        "            \"- Estimate clinical success probabilities (0.0â€“1.0)\\n\"\n",
        "            \"- Generate scientific summaries for Markdown or JSON reports\\n\\n\"\n",
        "\n",
        "            \"### How to Use It\\n\"\n",
        "            \"Wrap your query in triple backticks (```), starting with `TxGemmaChat`. Write your question on the next line.\\n\"\n",
        "            \"Do NOT include external instructions inside your prompt, only the direct biomedical content.\\n\\n\"\n",
        "\n",
        "            \"### Required Format\\n\"\n",
        "            \"```TxGemmaChat\\n\"\n",
        "            \"[your question, such as: What are the top 5 factors influencing success of Bemdaneprocel?]\\n\"\n",
        "            \"```\\n\\n\"\n",
        "\n",
        "            \"### Example:\\n\"\n",
        "            \"```TxGemmaChat\\n\"\n",
        "            \"Estimate clinical success probability for Bemdaneprocel based on trial metadata and literature.\\n\"\n",
        "            \"```\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "xh0mTC31FO1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_CHAT:\n",
        "    chat_tool = TxGemmaChatTool()\n",
        "    response = chat_tool.use_tool(\"Can Aspirin help with headaches? Yes or no?\")\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "VZL3FxB9FTlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"Single_Disease_Dataset_with_Index.csv\")\n",
        "\n",
        "row = df[\n",
        "    df['Stem-Cell Modality'].str.contains(\"bemdaneprocel\", case=False, na=False) &\n",
        "    df['Single_Disease'].str.contains(\"Parkinson\", case=False, na=False)\n",
        "].iloc[0]"
      ],
      "metadata": {
        "id": "2M2IGIagviVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PubMed Search\n",
        "\n",
        "! pip install --upgrade --quiet biopython\n",
        "\n",
        "from Bio import Medline, Entrez\n",
        "\n",
        "class PubMedSearch:\n",
        "    def __init__(self):\n",
        "        self.tool_name = \"PubMed Search\"\n",
        "\n",
        "    def tool_is_used(self, query: str):\n",
        "        return \"```PubMedSearch\" in query\n",
        "\n",
        "    def process_query(self, query: str):\n",
        "        search_text = extract_prompt(query, word=\"PubMedSearch\")\n",
        "        return search_text.strip()\n",
        "\n",
        "    def use_tool(self, search_text):\n",
        "        handle = Entrez.esearch(db=\"pubmed\", sort=\"relevance\", term=search_text, retmax=3)\n",
        "        record = Entrez.read(handle)\n",
        "        pmids = record.get(\"IdList\", [])\n",
        "        handle.close()\n",
        "\n",
        "        if not pmids:\n",
        "            return f\"No PubMed articles found for '{search_text}'. Please try a simpler search query.\"\n",
        "\n",
        "        fetch_handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pmids), rettype=\"medline\", retmode=\"text\")\n",
        "        records = list(Medline.parse(fetch_handle))\n",
        "        fetch_handle.close()\n",
        "\n",
        "        result_str = f\"=== PubMed Search Results for: '{search_text}' ===\\n\"\n",
        "        for i, record in enumerate(records, start=1):\n",
        "            pmid = record.get(\"PMID\", \"N/A\")\n",
        "            title = record.get(\"TI\", \"No title available\")\n",
        "            abstract = record.get(\"AB\", \"No abstract available\")\n",
        "            journal = record.get(\"JT\", \"No journal info\")\n",
        "            pub_date = record.get(\"DP\", \"No date info\")\n",
        "            authors = record.get(\"AU\", [])\n",
        "            authors_str = \", \".join(authors[:3])\n",
        "            result_str += (\n",
        "                f\"\\n--- Article #{i} ---\\n\"\n",
        "                f\"PMID: {pmid}\\n\"\n",
        "                f\"Title: {title}\\n\"\n",
        "                f\"Authors: {authors_str}\\n\"\n",
        "                f\"Journal: {journal}\\n\"\n",
        "                f\"Publication Date: {pub_date}\\n\"\n",
        "                f\"<abstract_start>{abstract}</abstract_finish>\\n\"\n",
        "            )\n",
        "        return f\"Query: {search_text}\\nResults: {result_str}\"\n",
        "\n",
        "    def instructions(self):\n",
        "        return (\n",
        "            f\"{'@' * 10}\\n@@@ PubMed Search Tool Instructions @@@\\n\\n\"\n",
        "            \"### What This Tool Does\\n\"\n",
        "            \"The PubMed Search Tool queries the NCBI Entrez API (PubMed) for a given search phrase, \"\n",
        "            \"and retrieves metadata for a few of the top articles (PMID, title, authors, journal, date, abstract).\\n\\n\"\n",
        "            \"### When / Why You Should Use It\\n\"\n",
        "            \"- To find **scientific literature** on biomedical topics like stem cell therapies.\\n\"\n",
        "            \"- To get **abstracts** as evidence for TxGemma prediction.\\n\\n\"\n",
        "            \"### Query Format\\n\"\n",
        "            \"Use triple backticks ``` and start with `PubMedSearch`. Example:\\n\"\n",
        "            \"```PubMedSearch\\nBemdaneprocel Parkinson's Disease\\n```\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "F4G6A19Evk9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_tool = PubMedSearch()\n",
        "search_results = pubmed_tool.use_tool(\"Bemdaneprocel Parkinson's disease stem cell therapy\")\n",
        "print(search_results)"
      ],
      "metadata": {
        "id": "RtrqwYXkMr5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a tool manager\n",
        "class ToolManager:\n",
        "    def __init__(self, toolset):\n",
        "        self.toolset = toolset\n",
        "\n",
        "    def tool_prompt(self):\n",
        "        # This will let the agent know what tools it has access to\n",
        "        tool_names = \", \".join([tool.tool_name for tool in self.toolset])\n",
        "        return f\"You have access to the following tools: {tool_names}\\n{self.tool_instructions()}. You can only use one tool at a time. These are the only tools you have access to nothing else.\"\n",
        "\n",
        "    def tool_instructions(self):\n",
        "        # This allows the agent to know how to use the tools\n",
        "        tool_instr = \"\\n\".join([tool.instructions() for tool in self.toolset])\n",
        "        return f\"The following is a set of instructions on how to use each tool.\\n{tool_instr}\"\n",
        "\n",
        "    def use_tool(self, query):\n",
        "        # This will iterate through all of the tools\n",
        "        # and find the correct tool that the agent requested\n",
        "        for tool in self.toolset:\n",
        "            if tool.tool_is_used(query):\n",
        "                # use the tool and return the output\n",
        "                return tool.use_tool(tool.process_query(query))\n",
        "        return f\"No tool match for search: {query}\"\n",
        "\n",
        "if USE_CHAT:\n",
        "    tools = ToolManager([TxGemmaChatTool(), PubMedSearch()])\n",
        "else:\n",
        "    tools = ToolManager([PubMedSearch()])"
      ],
      "metadata": {
        "id": "Jj_1pBS1StAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a Gemini inference tool\n",
        "\n",
        "def inference_gemini(prompt, system_prompt, model_str):\n",
        "  # Check to see that our model string matches\n",
        "  if model_str == \"gemini-2.5-pro\":\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-2.5-pro-preview-03-25\", system_instruction=system_prompt)\n",
        "    response = model.generate_content(prompt)\n",
        "    answer = response.text\n",
        "  return answer"
      ],
      "metadata": {
        "id": "w7QTNp0UTIox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a therapeutics agent\n",
        "\n",
        "class AgenticTx:\n",
        "    def __init__(self, tool_manager, model_str, num_steps=5):\n",
        "        self.curr_steps = 0\n",
        "        self.num_steps = num_steps\n",
        "        self.model_str = model_str\n",
        "        self.tool_manager = tool_manager\n",
        "        self.thoughts = list()\n",
        "        self.actions  = list()\n",
        "        self.observations = list()\n",
        "\n",
        "    def reset(self):\n",
        "        self.curr_steps = 0\n",
        "\n",
        "    def system_prompt(self, use_tools=True):\n",
        "        role_prompt = \"You are an expert therapeutic agent. You answer accurately and thoroughly.\"\n",
        "        prev_actions = f\"You can perform a maximum of {self.num_steps} actions. You have performed {self.curr_steps} and have {self.num_steps - self.curr_steps - 1} left.\"\n",
        "        if use_tools:\n",
        "            tool_prompt = \"You can use tools to solve problems and answer questions. \" + self.tool_manager.tool_prompt()\n",
        "        else:\n",
        "            tool_prompt = \"You cannot use any tools right now.\"\n",
        "        return f\"{role_prompt} {prev_actions} {tool_prompt}\"\n",
        "\n",
        "    def prior_information(self, query):\n",
        "        info_txt = f\"Question: {query}\\n\" if query is not None else \"\"\n",
        "        for _i in range(self.curr_steps):\n",
        "            info_txt += f\"### Thought {_i + 1}: {self.thoughts[_i]}\\n\"\n",
        "            info_txt += f\"### Action {_i + 1}: {self.actions[_i]}\\n\"\n",
        "            info_txt += f\"### Observation {_i + 1}: {self.observations[_i]}\\n\\n\"\n",
        "            info_txt += \"@\"*20\n",
        "        return info_txt\n",
        "\n",
        "    def step(self, question):\n",
        "        for _i in range(self.num_steps):\n",
        "            if self.curr_steps == self.num_steps - 1:\n",
        "                return inference_gemini(\n",
        "                    model_str=self.model_str,\n",
        "                    prompt=f\"{self.prior_information(question)}\\nYou must now provide an answer to this question {question}\",\n",
        "                    system_prompt=self.system_prompt(use_tools=False))\n",
        "            else:\n",
        "                thought = inference_gemini(\n",
        "                    model_str=self.model_str,\n",
        "                    prompt=f\"{self.prior_information(question)}\\nYou cannot currently use tools but you can think about the problem and what tools you want to use. This was the question, think about plans for how to use tools to answer this {question}. Let's think step by step (respond with only 1-2 sentences).\\nThought: \",\n",
        "                    system_prompt=self.system_prompt(use_tools=False))\n",
        "                action = inference_gemini(\n",
        "                    model_str=self.model_str,\n",
        "                    prompt=f\"{self.prior_information(question)}\\n{thought}\\nNow you must use tools to answer the following user query [{question}], closely following the tool instructions. Tool\",\n",
        "                    system_prompt=self.system_prompt(use_tools=True))\n",
        "                obs = self.tool_manager.use_tool(action)\n",
        "\n",
        "                print(\"Thought:\", thought)\n",
        "                print(\"Action:\", action)\n",
        "                print(\"Observation:\", obs)\n",
        "\n",
        "                self.thoughts.append(thought)\n",
        "                self.actions.append(action)\n",
        "                self.observations.append(obs)\n",
        "\n",
        "                self.curr_steps += 1"
      ],
      "metadata": {
        "id": "t_9Yt_QrUN1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentictx = AgenticTx(tool_manager=tools, model_str=\"gemini-2.5-pro\")\n",
        "\n",
        "trial_metadata = f\"\"\"\n",
        "<trial_metadata_start>\n",
        "Company: {row['Company (Website)']}\n",
        "HQ Country: {row['HQ Country']}\n",
        "Stem-Cell Modality: {row['Stem-Cell Modality']}\n",
        "Development Stage: {row['Development Stage']}\n",
        "Experimental vs. Formal: {row['Experimental vs. Formal']}\n",
        "Latest Funding: {row['Latest Funding (Date, Amount, Lead)']}\n",
        "Public/Private: {row['Public/Private']}\n",
        "Clinical Trials (NCT): {row['Clinical Trials (NCT)']}\n",
        "IP / Technology: {row['IP (Intellectual Property) / Technology']}\n",
        "Partnerships: {row['Partnerships/Collaborations']}\n",
        "Lead Disease Areas: {row['Lead Disease Areas']}\n",
        "Single Disease: {row['Single_Disease']}\n",
        "<trial_metadata_end>\n",
        "\"\"\"\n",
        "\n",
        "final_prompt = f\"\"\"Estimate the clinical success probability (range: 0.0â€“1.0) of Bemdaneprocel (BRTâ€‘DA01), list the top 5 contributing factors, and write natural language summary of prediction based on the following trial metadata.\n",
        "\n",
        "Respond ONLY with:\n",
        "- A numerical success score (0.0â€“1.0)\n",
        "- A list of the top 5 contributing factors\n",
        "- A concise natural language summary\n",
        "\n",
        "Do not restate the prompt or metadata.\n",
        "\n",
        "{trial_metadata}\n",
        "\"\"\"\n",
        "\n",
        "response = agentictx.step(final_prompt)\n",
        "print(\"\\nFinal Response:\", response)\n"
      ],
      "metadata": {
        "id": "LKjti1U9mVJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "f_md = open(\"bemdaneprocel_report.md\", \"w\")\n",
        "f_md.write(response)\n",
        "f_md.close()\n",
        "\n",
        "summary_json = {\n",
        "    \"task\": \"TX-009\",\n",
        "    \"timestamp\": str(datetime.datetime.now()),\n",
        "    \"response\": response\n",
        "}\n",
        "\n",
        "f_json = open(\"bemdaneprocel_report.json\", \"w\")\n",
        "json.dump(summary_json, f_json, indent=2)\n",
        "f_json.close()\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"bemdaneprocel_report.md\")\n",
        "files.download(\"bemdaneprocel_report.json\")"
      ],
      "metadata": {
        "id": "c3bZOuSBraqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8bbywkxCW_sr"
      }
    }
  ]
}